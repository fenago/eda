{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"A small caveat\nAbout 10 years ago when I was still in school, My professor taught us that EDA should be the first thing to do before doing anything else. He’s got a point back then because the dataset we were seeing and using had no more than 100 features. Today, EDA is still important in understanding your data but often not the first step you want to do. Since mostly you will end up dealing with thousands if not millions of features in your initial dataset and generating visualizations will end up being overwhelming. So I would highly recommend anyone who’s working on large data (features > 100) to do variable selection first before jumping into EDA.","metadata":{},"id":"d654a529-65ea-4fb0-8a5b-98d93f486afb"},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\n%matplotlib inline","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"679489de-71e6-4096-8936-2e2cec8b2a92"},{"cell_type":"code","source":"dfLE = pd.read_csv(\"https://raw.githubusercontent.com/fenago/introml/main/Life%20Expectancy%20Data.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"6b9fd3eb-1eef-47fb-930e-efac499be162"},{"cell_type":"code","source":"dfLE.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"77907dc6-04b9-4855-bca3-97fd5cb0d278"},{"cell_type":"code","source":"import ssl\nssl._create_default_https_context = ssl._create_unverified_context\nimport certifi","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"19b92dfd-ebc7-429a-81d1-ac24ffb44a28"},{"cell_type":"code","source":"from sklearn.datasets import fetch_openml","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"1d68d997-9c86-465b-a234-e1a62dec9f06"},{"cell_type":"code","source":"# Download the datasets from OPENML\ndf = fetch_openml(data_id=42803, as_frame=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"4c22361e-9649-48d7-9fe8-76d230146369"},{"cell_type":"code","source":"data = df","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"45ae9e09-49e6-434b-a860-10dfd34b629e"},{"cell_type":"code","source":"# Extract feature matrix X and show a few samples\ndf_X = data[\"frame\"]\ndf_X.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"8488a9ef-fd3e-4b4f-8f45-18b75ae23cd3"},{"cell_type":"code","source":"df_X.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"93afc81d-bc08-49cb-bfdb-35f4710c0616"},{"cell_type":"code","source":"pd.set_option('max_columns',None)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"e4ab795b-e2ef-45cc-abd9-6d5f406b2d88"},{"cell_type":"code","source":"df_X.nunique().to_frame().T","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"ecdd2a18-0d4b-4066-8023-518cf61a5c3b"},{"cell_type":"code","source":"# Count of how many times each data type is present in the dataset\npd.value_counts(df_X.dtypes)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"0a0fb220-1bf2-40fa-82af-b7a412c8b1d2"},{"cell_type":"code","source":"df_X.dtypes.to_frame().T","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"11d2e713-68a4-49ce-bc84-0b812d9a653c"},{"cell_type":"code","source":"df_X.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"36460053-d39e-4e6f-8ae8-f1838380cf40"},{"cell_type":"code","source":"df_X.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"9daa585e-c4e7-4ec7-93da-22cf7c85fb84"},{"cell_type":"code","source":"numerics = ['int16','int32','int64','float64']","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"0fc603e6-38e2-40a1-8467-eadd2a6673ec"},{"cell_type":"code","source":"catDF = df_X.select_dtypes(exclude=numerics)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"baad9fca-ae30-4c98-aaa3-43c7566b022d"},{"cell_type":"code","source":"numDF = df_X.select_dtypes(include=numerics)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"1e3f9a0d-46d6-412e-836c-f34ea66b52c5"},{"cell_type":"code","source":"catDF.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"84ae80ba-3621-4fb5-8dd9-4022cd532e84"},{"cell_type":"code","source":"numDF.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"f828a037-ccbb-4377-8b36-23d40929425c"},{"cell_type":"code","source":"# Merge back into a single df\n# preparing the X Variables  (Don't forget ot remove the target!!)\nX = pd.concat([catDF,numDF],axis=1)\nprint(X.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"b8955fbc-f97b-4bcb-95b4-d08bdb609753"},{"cell_type":"code","source":"y = df_X['Sex_of_Driver']","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"418435b0-b62e-49bb-8651-8628293e16b7"},{"cell_type":"code","source":"X.drop(['Sex_of_Driver'],axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"5f79226e-be24-4e06-968e-9d5e6cd640c9"},{"cell_type":"code","source":"# -------------------","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"f5eafde6-10c1-426c-a342-2aac72a6ecc6"},{"cell_type":"markdown","source":"# Categorical Features","metadata":{},"id":"db073a81-12dd-4f06-9119-8505632f5886"},{"cell_type":"code","source":"# Display non-numerical features\ndf_X.select_dtypes(exclude=\"number\").head()","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"01735193-4933-4ce0-8062-0d1b370ed01e"},{"cell_type":"markdown","source":"Even though Sex_of_Driver is a numerical feature, it somehow was stored as a non-numerical one. This is sometimes due to some typo in data recording. So let's take care of that:","metadata":{},"id":"27a38737-c54d-4d8b-867e-03ad32011c81"},{"cell_type":"code","source":"# Change data type of 'sex_of_Driver'\ndf_X['Sex_of_Driver'] = df_X['Sex_of_Driver'].astype('float')","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"d0c67b27-a0f1-4562-81dd-ebd71aa4dfcd"},{"cell_type":"markdown","source":"Using the .describe() function we can also investigate how many unique values each non-numerical feature has and with which frequency the most prominent value is present. When you pass exclude=”number” to df.describe, pandas excludes all the columns in the dataframe whose data types are subclasses of numpy.number\n\nonly the non-numeric columns remain in the dataframe. This is useful when you want to see only the categorical variables’ summary","metadata":{},"id":"63d7499c-005a-49d7-8e64-5dd8960ff804"},{"cell_type":"code","source":"df_X.describe(exclude='number')","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"f39bf0b7-e342-40b7-a96b-6163d754c57f"},{"cell_type":"markdown","source":"## Structure of numerical features\nNext, take a closer look at the numerical features. More precisely, investigate how many unique values each of these feature has. This process will give some insights about the number of binary (2 unique values), ordinal (3 to ~10 unique values) and continuous (more than 10 unique values) features in the dataset.","metadata":{},"id":"f03ae3c8-0f85-435e-93a8-d624b15eb65e"},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n# For each numerical feature compute number of unique entries\nunique_values = df_X.select_dtypes(include='number').nunique().sort_values()\nplt.figure(figsize=(15, 4))\nsns.set_style('whitegrid')\n\ng = sns.barplot(x=unique_values.index, y=unique_values, palette='inferno')\ng.set_yscale(\"log\")\ng.set_xticklabels(g.get_xticklabels(), rotation=45, horizontalalignment='right')\ng.set_title('Unique values per frequency')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"8615d0cd-d613-4263-a14f-0663ee24a766"},{"cell_type":"markdown","source":"Conclusion of structure investigation\nAt the end of this first investigation, we should have a better understanding of the general structure of our dataset. Number of samples and features, what kind of data type each feature has, and how many of them are binary, ordinal, categorical or continuous. For an alternative way to get such kind of information you could also use df_X.info() or df_X.describe().","metadata":{},"id":"53bd6450-db10-4bc6-8046-535d558ce383"},{"cell_type":"markdown","source":"# Quality Investigation\nBefore focusing on the actual content stored in these features, let’s first take a look at the general quality of the dataset. The goal is to have a global view on the dataset with regards to things like duplicates, missing values and unwanted entries or recording errors.\n\n## Duplicates\nDuplicates are entries that represent the same sample point multiple times. For example, if a measurement was registered twice by two different people. Detecting such duplicates is not always easy, as each dataset might have a unique identifier (e.g. an index number or recording time that is unique to each new sample) which you might want to ignore first.","metadata":{},"id":"8885a941-3071-4fd3-878f-cad7ed6051a6"},{"cell_type":"code","source":"# Check number of duplicates while ignoring the index feature\nn_duplicates = df_X.drop(labels=['Accident_Index'], axis=1).duplicated().sum()\n\nprint(f\"You seem to have {n_duplicates} duplicates in your database.\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"36c941f3-efcf-4a4b-a0de-986ec22da0a2"},{"cell_type":"markdown","source":"To handle these duplicates you can just simply drop them with .drop_duplicates().","metadata":{},"id":"36344a6f-5c98-4cd6-a83e-fa90e2407bb5"},{"cell_type":"code","source":"#  Extract column names of all features, except 'Accident_Index'\ncolumns_to_consider = df_X.drop(labels=['Accident_Index'], axis=1).columns\n\n# Drop duplicates based on 'columns_to_consider'\ndf_X.drop_duplicates(subset=columns_to_consider, inplace=True)\ndf_X.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"1bedc301-eba2-49c9-8dbe-39ab93f09e7a"},{"cell_type":"markdown","source":"## Missing Values\nAnother quality issue worth to investigate are missing values. Having some missing values is normal. What we want to identify at this stage are big holes in the dataset, i.e. samples or features with a lot of missing values.\n\n### Per Sample (Percentage missing per ROW) -- Look left to right for missing values!\nTo look at number of missing values per sample we have multiple options. The most straight forward one is to simply visualize the output of df_X.isna(), with something like this:","metadata":{},"id":"31f0654c-46d5-43ed-be86-17b3ce4eb0e7"},{"cell_type":"code","source":"plt.figure(figsize=(15, 8))\nsns.set_style('whitegrid')\n\ng = sns.heatmap(df_X.isnull(), cbar=False, cmap='viridis')\n# g = sns.heatmap(df_X.loc[df_X.isnull().sum(1).sort_values(ascending=1).index].isnull(), cbar=False, cmap='viridis')\ng.set_xlabel('Column Number')\ng.set_ylabel('Sample Number')","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"7ab32d22-9cc2-46e2-a0fd-d0eabc2a69a0"},{"cell_type":"markdown","source":"This figure shows on the y-axis each of the 360'000 individual samples, and on the x-axis if any of the 67 features contains a missing value. While this is already a useful plot, an even better approach is to use the missingno library, to get a plot like this one:","metadata":{},"id":"76893ed7-9eba-4adf-b4e5-0d23d9052dca"},{"cell_type":"code","source":"!pip install missingno\nimport missingno as msno\nmsno.matrix(df_X, labels=True, sort='descending', color=(0.27, 0.52, 1.0));","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"3d3dcf3c-d72a-45f0-b8c1-bab1a9a5dd2a"},{"cell_type":"markdown","source":"We can see that the dataset has a huge whole, caused by some samples where more than 50% of the feature values are missing. For those samples, filling the missing values with some replacement values is probably not a good idea.\n\nTherefore, let’s go ahead and drop samples that have more than 20% of missing values. The threshold is inspired by the information from the ‘Data Completeness’ column on the right of this figure.","metadata":{},"id":"f10e58ea-68bc-42d1-8a5e-da0a4831b38b"},{"cell_type":"code","source":"df_X = df_X.dropna(thresh=df_X.shape[1] * 0.80, axis=0).reset_index(drop=True)\ndf_X.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"03d57d37-5e05-484f-831d-e60bb1d75f64"},{"cell_type":"markdown","source":"### Per Feature (or Per Column) -- Look up and down!\nAs a next step, let’s now look at the number of missing values per feature. For this we can use some pandas trickery to quickly identify the ratio of missing values per feature.","metadata":{},"id":"7412bdf4-9836-447d-b06b-e4614fb6af3b"},{"cell_type":"code","source":"df_X.isna().mean().sort_values().plot(\n    kind=\"bar\", figsize=(15, 4),\n    title=\"Percentage of missing values per feature\",\n    ylabel=\"Ratio of missing values per feature\");","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"8204d4ef-1824-48a3-bbcf-45ef9b611d9b"},{"cell_type":"markdown","source":"From this figure we can see that most features don’t contain any missing values. Nonetheless, features like 2nd_Road_Class, Junction_Control, Age_of_Vehicle still contain quite a lot of missing values. So let's go ahead and remove any feature with more than 15% of missing values.","metadata":{},"id":"3457db3c-d216-443e-86d9-30c3c92804dc"},{"cell_type":"code","source":"df_X = df_X.dropna(thresh=df_X.shape[0] * 0.85, axis=1)\ndf_X.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"872e2635-6520-4e03-b627-5dc2221b1738"},{"cell_type":"markdown","source":"### Side Note\nMissing values: There is no strict order in removing missing values. For some datasets, tackling first the features and than the samples might be better. Furthermore, the threshold at which you decide to drop missing values per feature or sample changes from dataset to dataset, and depends on what you intend to do with the dataset later on.\n\nAlso, until now we only addressed the big holes in the dataset, not yet how we would fill the smaller gaps.","metadata":{},"id":"d34dc5ad-5a32-41dd-a639-fb4f73ad5dfd"},{"cell_type":"markdown","source":"## Unwanted entries and recording errors\nnother source of quality issues in a dataset can be due to unwanted entries or recording errors. It’s important to distinguish such samples from simple outliers. While outliers are data points that are unusual for a given feature distribution, unwanted entries or recording errors are samples that shouldn’t be there in the first place.\n\nFor example, a temperature recording of 45°C in Switzerland might be an outlier (as in ‘very unusual’), while a recording at 90°C would be an error. Similarly, a temperature recording from the top of Mont Blanc might be physical possible, but most likely shouldn’t be included in a dataset about Swiss cities.\n\nOf course, detecting such errors and unwanted entries and distinguishing them from outliers is not always straight forward and depends highly on the dataset. One approach to this is to take a global view on the dataset and see if you can identify some very unusual patterns.\n\n### Numerical Features\nTo plot this global view of the dataset, at least for the numerical features, you can use pandas’ .plot() function and combine it with the following parameters:\n\nlw=0: lw stands for line width. 0 means that we don't want to show any lines <br/>\nmarker=\".\": Instead of lines, we tell the plot to use . as markers for each data point<br/>\nsubplots=True: subplots tells pandas to plot each feature in a separate subplot<br/>\nlayout=(-1, 4): This parameter tells pandas how many rows and columns to use for the subplots. The -1 means \"as many as needed\", while the 2 means to use 2 columns per row.<br/>\nfigsize=(15, 30), markersize=1: To make sure that the figure is big enough we recommend to have a figure height of roughly the number of features, and to adjust the markersize accordingly.<br/>","metadata":{},"id":"22781bc6-fee2-42d6-8ff9-5f6c93cfa666"},{"cell_type":"code","source":"df_X.plot(lw=0, marker=\".\", subplots=True, layout=(-1, 4),\n          figsize=(15, 30), markersize=1);","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"e651d001-22b8-45b8-9ac5-15cac79b6541"},{"cell_type":"markdown","source":"Each point in this figure is a sample (i.e. a row) in our dataset and each subplot represents a different feature. The y-axis shows the feature value, while the x-axis is the sample index. These kind of plots can give you a lot of ideas for data cleaning and EDA. Usually it makes sense to invest as much time as needed until your happy with the output of this visualization.","metadata":{},"id":"684d2af0-138e-41b6-955e-4c8916497cfb"},{"cell_type":"markdown","source":"### Non-numerical features\nIdentifying unwanted entries or recording errors on non-numerical features is a bit more tricky. Given that at this point, we only want to investigate the general quality of the dataset. So what we can do is take a general look at how many unique values each of these non-numerical features contain, and how often their most frequent category is represented.","metadata":{},"id":"d3c99bbb-ceb1-4592-ba71-908337ab89cf"},{"cell_type":"code","source":"# Extract descriptive properties of non-numerical features\ndf_X.describe(exclude=[\"number\", \"datetime\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"bb08bcef-a9ea-4486-b6a7-014be1c432b8"},{"cell_type":"markdown","source":"There are multiple ways for how you could potentially streamline the quality investigation for each individual non-numerical features. None of them is perfect, and all of them will require some follow up investigation. But for the purpose of showcasing one such a solution, what we could do is loop through all non-numerical features and plot for each of them the number of occurrences per unique value.","metadata":{},"id":"8afbe5e4-1328-4fff-9bbf-f07b82006e3c"},{"cell_type":"code","source":"# Create figure object with 3 subplots\nfig, axes = plt.subplots(ncols=1, nrows=3, figsize=(12, 8))\n\n# Identify non-numerical features\ndf_non_numerical = df_X.select_dtypes(exclude=[\"number\", \"datetime\"])\n\n# Loop through features and put each subplot on a matplotlib axis object\nfor col, ax in zip(df_non_numerical.columns, axes.ravel()):\n\n    # Selects one single feature and counts number of occurrences per unique value\n    df_non_numerical[col].value_counts().plot(\n\n        # Plots this information in a figure with log-scaled y-axis\n        logy=True, title=col, lw=0, marker=\".\", ax=ax)\n    \nplt.tight_layout();","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"e5d9cad2-711d-434e-9801-49ca5d9aa00b"},{"cell_type":"markdown","source":"We can see that the most frequent accident (i.e. Accident_Index), had more than 100 people involved. Digging a bit deeper (i.e. looking at the individual features of this accident), we could identify that this accident happened on February 24th, 2015 at 11:55 in Cardiff UK. A quick internet search reveals that this entry corresponds to a luckily non-lethal accident including a minibus full of pensioners.\n\nThe decision for what should be done with such rather unique entries is once more left in the the subjective hands of the person analyzing the dataset. Without any good justification for WHY, and only with the intention to show you the HOW — let’s go ahead and remove the 10 most frequent accidents from this dataset.","metadata":{},"id":"5c1914b7-6cfd-4e22-9cd6-eadfef17b580"},{"cell_type":"code","source":"# Collect entry values of the 10 most frequent accidents\naccident_ids = df_non_numerical[\"Accident_Index\"].value_counts().head(10).index\n\n# Removes accidents from the 'accident_ids' list\ndf_X = df_X[~df_X[\"Accident_Index\"].isin(accident_ids)]\ndf_X.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"d9f3f082-8972-4cf6-a2a5-27cc8ca9179f"},{"cell_type":"markdown","source":"Conclusion of quality investigation\nAt the end of this second investigation, we should have a better understanding of the general quality of our dataset. We looked at duplicates, missing values and unwanted entries or recording errors. It is important to point out that we didn’t discuss yet how to address the remaining missing values or outliers in the dataset. This is a task for the next investigation, but won’t be covered just yet.","metadata":{},"id":"b8581fa9-5dee-4b0f-a30a-e1f9358d3c0b"},{"cell_type":"markdown","source":"# Content Investigation (This is the start of EDA)\nUp until now we only looked at the general structure and quality of the dataset. Let’s now go a step further and take a look at the actual content. In an ideal setting, such an investigation would be done feature by feature. But this becomes very cumbersome once you have more than 20–30 features.\n\nFor this reason (and to keep this NB as short for demonstration purposes) I’ll will explore three different approaches that can give you a very quick overview of the content stored in each feature and how they relate.\n\n## Feature distribution\nLooking at the value distribution of each feature is a great way to better understand the content of your data. Furthermore, it can help to guide your EDA, and provides a lot of useful information with regards to data cleaning and feature transformation. The quickest way to do this for numerical features is using histogram plots. Luckily, pandas comes with a builtin histogram function that allows the plotting of multiple features at once.","metadata":{},"id":"6a766275-fec8-4456-ba64-861820f5260c"},{"cell_type":"code","source":"# Plots the histogram for each numerical feature in a separate subplot\ndf_X.hist(bins=25, figsize=(15, 25), layout=(-1, 5), edgecolor=\"black\")\nplt.tight_layout();","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"4f59fabf-f816-4ece-a227-5604190d835b"},{"cell_type":"markdown","source":"There are a lot of very interesting things visible in this plot. For example…\n\nMost frequent entry: Some features, such as Towing_and_Articulation or Was_Vehicle_Left_Hand_Drive? mostly contain entries of just one category. Using the .mode() function, we could for example extract the ratio of the most frequent entry for each feature and visualize that information.","metadata":{},"id":"4055def2-cdf6-40ec-9f66-e7846d7a54dc"},{"cell_type":"code","source":"# Collects for each feature the most frequent entry\nmost_frequent_entry = df_X.mode()\n\n# Checks for each entry if it contains the most frequent entry\ndf_freq = df_X.eq(most_frequent_entry.values, axis=1)\n\n# Computes the mean of the 'is_most_frequent' occurrence\ndf_freq = df_freq.mean().sort_values(ascending=False)\n\n# Show the 5 top features with the highest ratio of singular value content\ndisplay(df_freq.head())\n\n# Visualize the 'df_freq' table\ndf_freq.plot.bar(figsize=(15, 4));","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"6c77d8f7-5a2e-414a-8367-4135352d46e1"},{"cell_type":"markdown","source":"Skewed value distributions: Certain kind of numerical features can also show strongly non-gaussian distributions. In that case you might want to think about how you can transform these values to make them more normal distributed. For example, for right skewed data you could use a log-transformation.\n\n## Feature patterns\nNext step on the list is the investigation of feature specific patterns. The goal of this part is two fold:\n\nCan we identify particular patterns within a feature that will help us to decide if some entries need to be dropped or modified?\nCan we identify particular relationships between features that will help us to better understand our dataset?\nBut before we dive into these two questions, let’s take a closer look at a few ‘randomly selected’ features.","metadata":{},"id":"4b3459b3-c609-442a-bae9-83fb1dc926b5"},{"cell_type":"code","source":"df_X[[\"Location_Northing_OSGR\", \"1st_Road_Number\",\n      \"Journey_Purpose_of_Driver\", \"Pedestrian_Crossing-Physical_Facilities\"]].plot(\n    lw=0, marker=\".\", subplots=True, layout=(-1, 2), markersize=0.1, figsize=(15, 6));","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"5e4c3eb5-bfdc-4348-afb0-1178f4f00b82"},{"cell_type":"markdown","source":"In the top row, we can see features with continuous values (e.g. seemingly any number from the number line), while in the bottom row we have features with discrete values (e.g. 1, 2, 3 but not 2.34).\n\nWhile there are many ways we could explore our features for particular patterns, let’s simplify our option by deciding that we treat features with less than 25 unique features as discrete or ordinal features, and the other features as continuous features.","metadata":{},"id":"34bb7ace-51d9-469b-99c2-2ae0547150fc"},{"cell_type":"code","source":"# Creates mask to identify numerical features with more or less than 25 unique features\ncols_continuous = df_X.select_dtypes(include=\"number\").nunique() >= 25","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"b0617645-1daf-44ef-bef2-3ea4a7feea55"},{"cell_type":"markdown","source":"### Continuous features\nNow that we have a way to select the continuous features, let’s go ahead and use seaborn’s pairplot to visualize the relationships between these features. Important to note, seaborn's pairplot routine can take a long time to create all subplots. Therefore we recommend to not use it for more than ~10 features at a time.","metadata":{},"id":"2bbadd43-7c7d-46c0-ad2e-11cf58a7aa9e"},{"cell_type":"code","source":"# Create a new dataframe which only contains the continuous features\ndf_continuous = df_X[cols_continuous[cols_continuous].index]\ndf_continuous.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"02b4bdd2-d351-460b-8b69-ef896a68afeb"},{"cell_type":"markdown","source":"Given that in our case we only have 11 features, we can go ahead with the pairplot. Otherwise, using something like df_continuous.iloc[:, :5] could help to reduce the number of features to plot.","metadata":{},"id":"37c71b4d-a8a6-49fd-b1f7-96766a37fa5b"},{"cell_type":"code","source":"sns.pairplot(df_continuous, height=1.5, plot_kws={\"s\": 2, \"alpha\": 0.2});","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"f0f65d12-c2b0-40dd-ab38-1e59688a116a"},{"cell_type":"markdown","source":"There seems to be a strange relationship between a few features in the top left corner. Location_Easting_OSGR and Longitude, as well as Location_Easting_OSGR and Latitude seem to have a very strong linear relationship.","metadata":{},"id":"cbcf79d2-f53f-4ec9-ae42-44b2c9656fd9"},{"cell_type":"code","source":"g = sns.pairplot(\n    df_X, plot_kws={'s': 3, 'alpha': 0.2}, hue='Police_Force', palette='Spectral',\n    x_vars=['Location_Easting_OSGR', 'Location_Northing_OSGR', 'Longitude'],\n    y_vars='Latitude');\ng.fig.set_size_inches(15,8)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"1451b2da-8059-444d-8545-cfe8d160c8c9"},{"cell_type":"markdown","source":"Knowing that these features contain geographic information, a more in-depth EDA with regards to geolocation could be fruitful. However, for now we will leave the further investigation of this pairplot to the curious reader and continue with the exploration of the discrete and ordinal features.\n\n### Discrete and ordinal features\nFinding patterns in the discrete or ordinal features is a bit more tricky. But also here, some quick pandas and seaborn trickery can help us to get a general overview of our dataset. First, let’s select the columns we want to investigate.","metadata":{},"id":"4eb4578d-bb33-4fb2-b554-0aef12551dc7"},{"cell_type":"code","source":"# Create a new dataframe which doesn't contain the numerical continuous features\ndf_discrete = df_X[cols_continuous[~cols_continuous].index]\ndf_discrete.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"eeb716b7-ea5c-403b-bccd-d3fc5e635105"},{"cell_type":"markdown","source":"As always, there are multiple way for how we could investigate all of these features. Let’s try one example, using seaborn’s stripplot() together with a handy zip() for-loop for subplots.\n\nNote, to spread the values out in the direction of the y-axis we need to chose one particular (hopefully informative) feature. While the ‘right’ feature can help to identify some interesting patterns, usually any continuous feature should do the trick. The main interest in this kind of plot is to see how many samples each discrete value contains.","metadata":{},"id":"4f5ef754-4798-42bf-99fd-97f1e5bb6622"},{"cell_type":"code","source":"# Establish number of columns and rows needed to plot all features\nn_cols = 5\nn_elements = len(df_discrete.columns)\nn_rows = np.ceil(n_elements / n_cols).astype(\"int\")\n\n# Specify y_value to spread data (ideally a continuous feature)\ny_value = df_X[\"Age_of_Driver\"]\n\n# Create figure object with as many rows and columns as needed\nfig, axes = plt.subplots(ncols=n_cols, nrows=n_rows, figsize=(15, n_rows * 2.5))\n\n# Loop through features and put each subplot on a matplotlib axis object\nfor col, ax in zip(df_discrete.columns, axes.ravel()):\n    sns.stripplot(data=df_X, x=col, y=y_value, ax=ax, palette=\"tab10\", size=1, alpha=0.5)\nplt.tight_layout();","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"1ff660c2-ace1-4cd3-b581-d60f9eb118f6"},{"cell_type":"markdown","source":"There are too many things to comment here, so let’s just focus on a few. In particular, let’s focus on 6 features where the values appear in some particular pattern or where some categories seem to be much less frequent than others. And to shake things up a bit, let’s now use the Longitude feature to stretch the values over the y-axis.","metadata":{},"id":"c3783a73-20ed-4e1c-a89a-ad348a1634bc"},{"cell_type":"code","source":"# Specify features of interest\nselected_features = [\"Vehicle_Reference_df_res\", \"Towing_and_Articulation\",\n                     \"Skidding_and_Overturning\", \"Bus_or_Coach_Passenger\",\n                     \"Pedestrian_Road_Maintenance_Worker\", \"Age_Band_of_Driver\"]\n\n# Create a figure with 3 x 2 subplots\nfig, axes = plt.subplots(ncols=3, nrows=2, figsize=(16, 8))\n\n# Loop through these features and plot entries from each feature against `Latitude`\nfor col, ax in zip(selected_features, axes.ravel()):\n    sns.stripplot(data=df_X, x=col, y=df_X[\"Latitude\"], ax=ax,\n                  palette=\"tab10\", size=2, alpha=0.5)\nplt.tight_layout();","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"a2170ff1-bfe9-4cbd-89d4-542c2a6439d5"},{"cell_type":"markdown","source":"These kind of plots are already very informative, but they obscure regions where there are a lot of data points at once. For example, there seems to be a high density of points in some of the plots at the 52nd latitude. So let’s take a closer look with an appropriate plot, such as violineplot ( or boxenplot or boxplot for that matter). And to go a step further, let's also separate each visualization by Urban_or_Rural_Area.","metadata":{},"id":"dbcf7290-8f15-40ff-bbe7-7bdace73c5a2"},{"cell_type":"code","source":"# Create a figure with 3 x 2 subplots\nfig, axes = plt.subplots(ncols=3, nrows=2, figsize=(16, 8))\n\n# Loop through these features and plot entries from each feature against `Latitude`\nfor col, ax in zip(selected_features, axes.ravel()):\n    sns.violinplot(data=df_X, x=col, y=df_X[\"Latitude\"], palette=\"Set2\",\n                   split=True, hue=\"Urban_or_Rural_Area\", ax=ax)\nplt.tight_layout();","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"7d24d7c8-7b1b-41b8-a684-7d532e9b86e4"},{"cell_type":"markdown","source":"Interesting! We can see that some values on features are more frequent in urban, than in rural areas (and vice versa). Furthermore, as suspected, there seems to be a high density peak at latitude 51.5. This is very likely due to the more densely populated region around London (at 51.5074°).","metadata":{},"id":"d315ce57-43ec-4cfe-8487-18cc66c840b2"},{"cell_type":"markdown","source":"## Feature relationships\nLast, but not least, let’s take a look at relationships between features. More precisely how they correlate. The quickest way to do so is via pandas’ .corr() function. So let's go ahead and compute the feature to feature correlation matrix for all numerical features.","metadata":{},"id":"53bf402e-4729-49a6-958c-23b4c02e214b"},{"cell_type":"code","source":"# Computes feature correlation\ndf_corr = df_X.corr(method=\"pearson\")","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"8ce3c283-fbe3-4e0c-a817-62646b488914"},{"cell_type":"markdown","source":"Note: Depending on the dataset and the kind of features (e.g. ordinal or continuous features) you might want to use the spearman method instead of the pearson method to compute the correlation. Whereas the Pearson correlation evaluates the linear relationship between two continuous variables, the Spearman correlation evaluates the monotonic relationship based on the ranked values for each feature. And to help with the interpretation of this correlation matrix, let's use seaborn's .heatmap() to visualize it.","metadata":{},"id":"79a7d15e-0da9-4ca0-a748-af55862122de"},{"cell_type":"code","source":"# Create labels for the correlation matrix\nlabels = np.where(np.abs(df_corr)>0.75, \"S\",\n                  np.where(np.abs(df_corr)>0.5, \"M\",\n                           np.where(np.abs(df_corr)>0.25, \"W\", \"\")))\n\n# Plot correlation matrix\nplt.figure(figsize=(15, 15))\nsns.heatmap(df_corr, mask=np.eye(len(df_corr)), square=True,\n            center=0, annot=labels, fmt='', linewidths=.5,\n            cmap=\"vlag\", cbar_kws={\"shrink\": 0.8});","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"27e2d12b-ecb5-4deb-bbf2-99c13a8bb6b7"},{"cell_type":"markdown","source":"This looks already very interesting. We can see a few very strong correlations between some of the features. Now, if you’re interested actually ordering all of these different correlations, you could do something like this:","metadata":{},"id":"9fd9d758-0b2e-4c00-aa0b-422063d37fb3"},{"cell_type":"code","source":"#  Creates a mask to remove the diagonal and the upper triangle.\nlower_triangle_mask = np.tril(np.ones(df_corr.shape), k=-1).astype(\"bool\")\n\n#  Stack all correlations, after applying the mask\ndf_corr_stacked = df_corr.where(lower_triangle_mask).stack().sort_values()\n\n#  Showing the lowest and highest correlations in the correlation matrix\ndisplay(df_corr_stacked)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"35744fa1-7452-4d65-bcd2-abe8a672e46d"},{"cell_type":"markdown","source":"As you can see, the investigation of feature correlations can be very informative. But looking at everything at once can sometimes be more confusing than helpful. So focusing only on one feature with something like df_X.corrwith(df_X[\"Speed_limit\"]) might be a better approach.\n\nFurthermore, correlations can be deceptive if a feature still contains a lot of missing values or extreme outliers. Therefore, it is always important to first make sure that your feature matrix is properly prepared before investigating these correlations.\n\n## Conclusion of content investigation\nAt the end of this third investigation, we should have a better understanding of the content in our dataset. We looked at value distribution, feature patterns and feature correlations. However, these are certainly not all possible content investigation and data cleaning steps you could do. Additional steps would for example be outlier detection and removal, feature engineering and transformation, and more.\n\n# In Conclusion\nA proper and detailed EDA takes time! It is a very iterative process that often makes you go back to the start, after you addressed another flaw in the dataset. This is normal! It’s the reason why we often say that 80% of any data science project is data preparation and EDA.\n\nBut keep also in mind that an in-depth EDA can consume a lot of time. And just because something seems interesting doesn’t mean that you need to follow up on it. Always remind yourself what the dataset will be used for and tailor your investigations to support that goal. And sometimes it is also ok, to just do a quick-and-dirty data preparation and exploration. So that you can move on to the data modeling part rather quickly, and to establish a few preliminary baseline models perform some informative results investigation.","metadata":{},"id":"cc115c56-75c6-4235-b5d8-bfc7cba7c694"},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[],"id":"c02024ff-2fad-4fc6-88dd-5620e710f553"}]}